{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example RNN implementation using Keras\n",
    "**Credit: Chenhao Tan**, code obtained from [CSCI 4622](https://github.com/BoulderDS/CSCI-4622-Machine-Learning-18fa) at CU Boulder\n",
    "\n",
    "Dataset is obtained from UCI Machine Learning repository consisting of SMS tagged messages (being ham (legitimate) or spam) that have been collected for SMS Spam research.\n",
    "\n",
    "Use [Keras](https://keras.io/) to implement a classifier (you need to install Keras). Update the below snippet to build a Sequential model with an embedding layer, and an LSTM layer followed by a dense layer. This question allows you to get familiar with popular deep learning toolkits and the solution only has a few lines. In practice, there is no need to reinvent the wheels.\n",
    "\n",
    "Learn more about RNN : https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "Note: TensorFlow not supported on Python 3.7 so use a Python virtual environment with Python 3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "class RNN:\n",
    "    '''\n",
    "    RNN classifier\n",
    "    '''\n",
    "\n",
    "    def __init__(self, train_x, train_y, test_x, test_y, dict_size=5000,\n",
    "                 example_length=150, embedding_length=32, epoches=5, batch_size=128):\n",
    "        '''\n",
    "        initialize RNN model\n",
    "        :param train_x: training data\n",
    "        :param train_y: training label\n",
    "        :param test_x: test data\n",
    "        :param test_y: test label\n",
    "        :param epoches: number of ephoches to run\n",
    "        :param batch_size: batch size in training\n",
    "        :param embedding_length: size of word embedding\n",
    "        :param example_length: length of examples\n",
    "        '''\n",
    "        self.batch_size = batch_size\n",
    "        self.epoches = epoches\n",
    "        self.example_len = example_length\n",
    "        self.dict_size = dict_size\n",
    "        self.embedding_len = embedding_length\n",
    "\n",
    "        # preprocess training data\n",
    "        tok = Tokenizer(num_words=dict_size)\n",
    "        tok.fit_on_texts(train_x)\n",
    "        sequences = tok.texts_to_sequences(train_x)\n",
    "        self.train_x = sequence.pad_sequences(\n",
    "            sequences, maxlen=self.example_len)\n",
    "        sequences = tok.texts_to_sequences(test_x)\n",
    "        self.test_x = sequence.pad_sequences(\n",
    "            sequences, maxlen=self.example_len)\n",
    "\n",
    "        self.train_y = train_y\n",
    "        self.test_y = test_y\n",
    "        \n",
    "        print(train_x)\n",
    "\n",
    "        # TODO: build model with Embedding, LSTM and dense layers.\n",
    "        # refer to Sequence classification with LSTM : https://keras.io/getting-started/sequential-model-guide/#examples\n",
    "        # Documentation for LSTM layer in : https://keras.io/layers/recurrent/#lstm\n",
    "        self.model = Sequential()\n",
    "        # YOUR CODE HERE\n",
    "        self.model.add(Embedding(self.dict_size, self.embedding_len, input_length=self.example_len))\n",
    "        self.model.add(LSTM(self.embedding_len))\n",
    "        self.model.add(Dense(1, activation='sigmoid'))\n",
    "        self.model.compile(loss='binary_crossentropy',\n",
    "                           optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "        print(self.model.summary())\n",
    "\n",
    "    def train(self, verbose=0):\n",
    "        '''\n",
    "        fit in data and train model : refer fit method in https://keras.io/models/model/\n",
    "        make sure you use batchsize and epochs appropriately.\n",
    "        :return:None\n",
    "        '''\n",
    "        # TODO: fit in data to train your model\n",
    "        # YOUR CODE HERE\n",
    "        self.model.fit(self.train_x, self.train_y, batch_size=self.batch_size, epochs=self.epoches)\n",
    "\n",
    "    def evaluate(self):\n",
    "        '''\n",
    "\n",
    "        evaluate trained model : Please refer evaluate in https://keras.io/models/model/\n",
    "        :return: [loss,accuracy]\n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        return self.model.evaluate(self.train_x, self.train_y)\n",
    "    \n",
    "    def evaluate_test(self):\n",
    "        return self.model.evaluate(self.test_x, self.test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def load_data(location):\n",
    "    return pickle.load(open(location,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = load_data('./data/spam_data.pkl')\n",
    "rnn = RNN(train_x, train_y, test_x, test_y, epoches=5)\n",
    "rnn.train(verbose=1)\n",
    "accuracy = rnn.evaluate()\n",
    "print('Accuracy for LSTM: ', accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
